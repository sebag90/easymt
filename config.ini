[DATA]
src_train = data/ita_sp_35/train.it
tgt_train = data/ita_sp_35/train.en
src_eval = data/ita_sp_35/eval.it
tgt_eval = data/ita_sp_35/eval.en
src_vocab = data/ita_sp_35/vocab.it
tgt_vocab = data/ita_sp_35/vocab.en

[MODEL]
type = transformer
source = it
target = en
encoder_layers = 2
decoder_layers = 1
max_length = 256
uniform_init = 0.1
shared_embedding = False

[RNN]
hidden_size = 300
word_vec_size = 256
attention = general
bidirectional = True
rnn_dropout = 0.3
attn_dropout = 0.1
input_feed = True

[TRANSFORMER]
d_model = 256
heads = 8
dim_feedforward = 300
attn_dropout = 0.2
residual_dropout = 0.1

[TRAINING]
print_every = 10
steps = 100000
step_size = 32
batch_size = 8
optimizer = noam
learning_rate = 0.001
lr_reducing_factor = 0.5
noam_factor = 2
warm_up = 4000
patience = 2
save_every = 0
teacher = True
teacher_ratio = 0.5
valid_steps = 20000
label_smoothing = 0.1
gradient_clipping = 5