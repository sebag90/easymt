[MODEL]
type = transformer
source = it
target = en
encoder_layers = 2
decoder_layers = 1
max_length = 256
uniform_init = 0.1

[RNN]
hidden_size = 300
word_vec_size = 256
attention = general
bidirectional = True
rnn_dropout = 0.3
attn_dropout = 0.1
input_feed = True

[TRANSFORMER]
d_model = 256
heads = 4
dim_feedforward = 300
attn_dropout = 0.2
residual_dropout = 0.1
noam_factor = 2
warm_up = 4000

[TRAINING]
print_every = 100
steps = 100000
batch_size = 32
optimizer = noam
learning_rate = 0.001
lr_reducing_factor = 0.5
patience = 2
save_every = 0
teacher = True
teacher_ratio = 0.5
valid_steps = 25000
label_smoothing = 0.1