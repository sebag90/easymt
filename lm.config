[DATA]
src_train = data/en.train
tgt_train = data/en.train
src_eval = data/en.eval
tgt_eval = data/en.eval
src_vocab = data/vocab.en
tgt_vocab = data/vocab.en

[MODEL]
task = language generation
type = transformer
source = en
target = it
encoder_layers = 2
decoder_layers = 2
max_length = 512
uniform_init = 0.1
shared_embedding = False

[RNN]
hidden_size = 300
word_vec_size = 256
attention = general
bidirectional = True
rnn_dropout = 0.3
attn_dropout = 0.1
input_feed = True

[TRANSFORMER]
d_model = 256
heads = 8
dim_feedforward = 512
attn_dropout = 0.1
residual_dropout = 0.1

[TRAINING]
print_every = 1
steps = 100000
batch_size = 4
step_size = 32
optimizer = noam
learning_rate = 0.001
lr_reducing_factor = 0.5
noam_factor = 2
warm_up = 4000
patience = 2
save_every = 10000
teacher = True
teacher_ratio = 0.5
valid_steps = 20000
label_smoothing = 0.1
gradient_clipping = 1